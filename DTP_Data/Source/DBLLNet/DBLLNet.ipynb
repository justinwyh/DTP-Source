{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DBLLNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmXc3vLv4eDk",
        "colab_type": "code",
        "outputId": "c7271e87-7205-4645-b1f3-93dd670e9e51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "import os\n",
        "import math\n",
        "\n",
        "!git clone https://github.com/jorge-pessoa/pytorch-msssim.git\n",
        "%cd pytorch-msssim/\n",
        "!python setup.py install\n",
        "from pytorch_msssim import ssim, msssim\n",
        "\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['figure.dpi']= 100"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pytorch-msssim'...\n",
            "remote: Enumerating objects: 136, done.\u001b[K\n",
            "remote: Total 136 (delta 0), reused 0 (delta 0), pack-reused 136\u001b[K\n",
            "Receiving objects: 100% (136/136), 1.24 MiB | 3.58 MiB/s, done.\n",
            "Resolving deltas: 100% (58/58), done.\n",
            "/content/pytorch-msssim/pytorch-msssim\n",
            "running install\n",
            "running build\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/pytorch_msssim\n",
            "copying pytorch_msssim/__init__.py -> build/lib/pytorch_msssim\n",
            "running install_lib\n",
            "copying build/lib/pytorch_msssim/__init__.py -> /usr/local/lib/python3.6/dist-packages/pytorch_msssim\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/pytorch_msssim/__init__.py to __init__.cpython-36.pyc\n",
            "running install_egg_info\n",
            "Removing /usr/local/lib/python3.6/dist-packages/pytorch_msssim-0.1.egg-info\n",
            "Writing /usr/local/lib/python3.6/dist-packages/pytorch_msssim-0.1.egg-info\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxXdS05dxZTm",
        "colab_type": "text"
      },
      "source": [
        "**Mount drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBWSpb9PxcLs",
        "colab_type": "code",
        "outputId": "065e3b33-76b1-44f1-98cd-63c07a782ace",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkUtHZmYxFun",
        "colab_type": "text"
      },
      "source": [
        "**JIT Load Custom Op**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "og5DIVZ5xBms",
        "colab_type": "code",
        "outputId": "28615350-ed3d-4633-9964-a9d44fdc33cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install ninja\n",
        "from torch.utils.cpp_extension import load\n",
        "op_path = '/content/gdrive/My Drive/bilateral_slice_op/'\n",
        "bsliceapply = load(name='bilateral_slicing', sources=[os.path.join(op_path,'bilateral_slicing.cpp'), os.path.join(op_path,'bilteral_slicing_kernel.cu')])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ninja in /usr/local/lib/python3.6/dist-packages (1.9.0.post1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpi6eVyH4Wak",
        "colab_type": "text"
      },
      "source": [
        "**Layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kN0f_6SkvyDN",
        "colab": {}
      },
      "source": [
        "class Convolutional_Layer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding = 1, activation=nn.ReLU, batch_norm=False, bias=True):\n",
        "        super(Convolutional_Layer, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,stride=stride, padding=padding, bias=bias)\n",
        "        self.batch_norm = nn.BatchNorm2d(out_channels) if batch_norm else None\n",
        "        self.activation = None if activation is None else activation()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        if self.batch_norm is not None:\n",
        "            out = self.batch_norm(out)\n",
        "        if self.activation is not None:\n",
        "            out = self.activation(out)\n",
        "        #print(\"Conv\" , out.shape)\n",
        "        return out\n",
        "\n",
        "class Fully_Connected_Layer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, activation=nn.ReLU, batch_norm=False, bias=True):\n",
        "        super(Fully_Connected_Layer, self).__init__()\n",
        "        self.FC = nn.Linear(in_features, out_features, bias=bias)\n",
        "        self.batch_norm = nn.BatchNorm1d(out_features) if batch_norm else None\n",
        "        self.activation = None if activation is None else activation()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.FC(x)\n",
        "        if self.batch_norm is not None:\n",
        "            out = self.batch_norm(out)\n",
        "        if self.activation is not None:\n",
        "            out = self.activation(out)\n",
        "        #print(\"FC\", out.shape)\n",
        "        return out\n",
        "\n",
        "class Slicing_Apply_Function(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, grid, guide, frinput):\n",
        "        has_offset = True;\n",
        "        output = bsliceapply.forward(grid, guide, frinput, has_offset)\n",
        "        ctx.save_for_backward(grid, guide, frinput)\n",
        "        ctx.offset = has_offset\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad):\n",
        "        grid, guide, frinput = ctx.saved_tensors\n",
        "        has_offset = ctx.offset\n",
        "        outputs = bsliceapply.backward(grid, guide, frinput, grad, has_offset)\n",
        "        grad_grid, grad_guide, grad_frinput = outputs\n",
        "        return grad_frinput, grad_guide, grad_grid, None\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0rb76kd2kxh",
        "colab_type": "text"
      },
      "source": [
        "**Grad Check**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoHXGDsG2mpH",
        "colab_type": "code",
        "outputId": "0c99d55f-3896-4eec-9b12-28d4a17a2b51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from torch.autograd import gradcheck\n",
        "\n",
        "grid = torch.rand(2, 12, 8, 3, 3,dtype=torch.double, requires_grad=True).cuda()\n",
        "guide =  torch.rand(2, 3, 3, dtype=torch.double, requires_grad=True).cuda()\n",
        "frinput =  torch.rand(2, 3, 3, 3,dtype=torch.double, requires_grad=True).cuda()\n",
        "\n",
        "\n",
        "def grad_test(grid, guide, frinput):\n",
        "  return Slicing_Apply_Function.apply(grid, guide, frinput)\n",
        "\n",
        "is_grad_correct = gradcheck(grad_test, [grid, guide, frinput], eps=1e-3, atol=1e1, raise_exception=True)\n",
        "print(is_grad_correct)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MajLoyqB4iGM",
        "colab_type": "text"
      },
      "source": [
        "**Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRVEXq_eHHBr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Lr_Splat(nn.Module):  # Extract low-level features\n",
        "    def __init__(self, cm, sb, lb, bn, lris):\n",
        "        super(Lr_Splat, self).__init__()\n",
        "        n_conv_layers = int(np.log2(lris / sb))  # Number of conv layers required to reduce the spatial size to sb\n",
        "        self.splat_layers = nn.ModuleList()\n",
        "        in_channels = 3\n",
        "        for i in range(n_conv_layers):\n",
        "            b_n = bn if i > 0 else False\n",
        "            out_channels = cm * (2 ** i) * lb\n",
        "            self.splat_layers.append(Convolutional_Layer(in_channels=in_channels,\n",
        "                                                         out_channels=out_channels,\n",
        "                                                         kernel_size=3, stride=2, batch_norm=b_n))\n",
        "            in_channels = out_channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x\n",
        "        for layer in self.splat_layers:\n",
        "            out = layer(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Lr_LocalFeatures(nn.Module):  # Local features in low-res stream\n",
        "    def __init__(self, cm, sb, lb, bn, lris):\n",
        "        super(Lr_LocalFeatures, self).__init__()\n",
        "        n_lr_splat_channels = int(cm * (2 ** int(np.log2(lris / sb) - 1)) * lb)\n",
        "        self.lf_layers = nn.ModuleList()\n",
        "        b_n = bn if bn else False\n",
        "        self.lf_layers.append(Convolutional_Layer(in_channels=n_lr_splat_channels,\n",
        "                                                  out_channels=n_lr_splat_channels,\n",
        "                                                  kernel_size=3, stride=1, batch_norm=b_n))\n",
        "        self.lf_layers.append(Convolutional_Layer(in_channels=n_lr_splat_channels,\n",
        "                                                  out_channels=n_lr_splat_channels,\n",
        "                                                  kernel_size=3, stride=1, activation=None))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x\n",
        "        for layer in self.lf_layers:\n",
        "            out = layer(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Lr_GlobalFeatures(nn.Module):  # Global features in low-res stream\n",
        "    def __init__(self, cm, sb, lb, bn, lris):\n",
        "        super(Lr_GlobalFeatures, self).__init__()\n",
        "        n_lr_splat_channels = int(cm * (2 ** int(np.log2(lris / sb) - 1)) * lb)\n",
        "        n_splat_conv_layers = int(np.log2(lris / sb))\n",
        "        n_lrgf_conv_layers = int(np.log2(sb / 4))\n",
        "        self.gf_conv_layers = nn.ModuleList()\n",
        "        self.gf_fc_layers = nn.ModuleList()\n",
        "        b_n = bn if bn else False\n",
        "        #Convolution Layers\n",
        "        for i in range(n_lrgf_conv_layers):\n",
        "            self.gf_conv_layers.append(Convolutional_Layer(in_channels=n_lr_splat_channels,\n",
        "                                                           out_channels=n_lr_splat_channels,\n",
        "                                                           kernel_size=3, stride=2, batch_norm=b_n))\n",
        "        #Fully Connected Layers\n",
        "        n_prev_layer_size = int((lris / 2 ** (n_splat_conv_layers + n_lrgf_conv_layers)) ** 2)\n",
        "        self.gf_fc_layers.append(Fully_Connected_Layer(in_features=n_prev_layer_size * n_lr_splat_channels,\n",
        "                                                       out_features=32 * cm * lb,\n",
        "                                                       batch_norm=b_n))\n",
        "        self.gf_fc_layers.append(Fully_Connected_Layer(in_features=32 * cm * lb,\n",
        "                                                       out_features=16 * cm * lb,\n",
        "                                                       batch_norm=b_n))\n",
        "        self.gf_fc_layers.append(Fully_Connected_Layer(in_features=16 * cm * lb,\n",
        "                                                       out_features=8 * cm * lb, activation=None))\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = x\n",
        "        for layer in self.gf_conv_layers:\n",
        "            out = layer(out)\n",
        "        out = out.view(list(out.size())[0],-1) #keep batch size\n",
        "        #print(out.shape)\n",
        "        for layer in self.gf_fc_layers:\n",
        "            out = layer(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class Fusion(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Fusion, self).__init__()\n",
        "        self.Relu = nn.ReLU()\n",
        "\n",
        "    def forward(self,LrLocalFeatures, LrGlobalFeatures):\n",
        "      Rs_LrGlobalFeatures = LrGlobalFeatures.view(list(LrGlobalFeatures.size())[0],list(LrGlobalFeatures.size())[1], 1, 1) #Pytorch: [batch size, channel, size, size]\n",
        "      #print(Rs_LrGlobalFeatures.shape)\n",
        "      out = torch.add(LrLocalFeatures, Rs_LrGlobalFeatures)\n",
        "      out = self.Relu(out)\n",
        "      #print(out.shape)\n",
        "      return out\n",
        "\n",
        "class LinearPredict_BGrid(nn.Module):\n",
        "    def __init__(self,cm, lb, nin=4, nout=3):\n",
        "        super(LinearPredict_BGrid, self).__init__()\n",
        "        self.lb = lb\n",
        "        self.conv = Convolutional_Layer(in_channels=8 * cm * lb, out_channels= lb * nin * nout,\n",
        "                                        kernel_size=1, stride=1, padding=0,activation=None) #No batch norm\n",
        "\n",
        "    def forward(self,x):\n",
        "        batch_size = list(x.size())[0]\n",
        "        out = x\n",
        "        out = self.conv(out) # [batch_size, 96, 16, 16]\n",
        "        out = torch.stack(tensors = torch.split(tensor=out,split_size_or_sections=self.lb,dim=1),dim = 1) #unroll grid\n",
        "        #print(out.shape)\n",
        "        return out\n",
        "\n",
        "class Guide_PointwiseNN(nn.Module):\n",
        "    def __init__(self, bn,guide_complexity=16):\n",
        "      super(Guide_PointwiseNN, self).__init__()\n",
        "      b_n = bn if bn else False\n",
        "      self.conv1 = Convolutional_Layer(in_channels=3, out_channels=guide_complexity,\n",
        "                                        kernel_size=1, stride= 1,padding=0,batch_norm=b_n)\n",
        "      self.conv2 = Convolutional_Layer(in_channels=guide_complexity,out_channels=1,\n",
        "                                        kernel_size=1,stride=1, padding=0, activation=nn.Sigmoid)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x\n",
        "        out = self.conv1(out)\n",
        "        out = self.conv2(out)\n",
        "        return out.squeeze(1)\n",
        "\n",
        "class SliceNApply(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(SliceNApply, self).__init__()\n",
        "   \n",
        "  def forward(self,Bilterial_Grid,Guide,fr):\n",
        "      return Slicing_Apply_Function.apply(Bilterial_Grid,Guide,fr)\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self,_cm,_sb,_lb,_bn,_lris):\n",
        "    super(Net,self).__init__()\n",
        "    self.splat = Lr_Splat(cm=_cm,sb=_sb,lb=_lb,bn=_bn,lris=_lris)\n",
        "    self.localf = Lr_LocalFeatures(cm=_cm,sb=_sb,lb=_lb,bn=_bn,lris=_lris)\n",
        "    self.globalf = Lr_GlobalFeatures(cm=_cm,sb=_sb,lb=_lb,bn=_bn,lris=_lris)\n",
        "    self.fusion = Fusion()\n",
        "    self.bgrid = LinearPredict_BGrid(cm=_cm,lb=_lb)\n",
        "    self.guide = Guide_PointwiseNN(bn=_bn)\n",
        "    self.slice_op = SliceNApply()\n",
        "  \n",
        "  def forward(self,lr, fr):\n",
        "    out = self.splat(lr)\n",
        "    local_out = self.localf(out)\n",
        "    global_out = self.globalf(out)\n",
        "    fus_out = self.fusion(local_out,global_out)\n",
        "    bg_out = self.bgrid(fus_out)\n",
        "    g_out = self.guide(fr)\n",
        "    fin_out = self.slice_op(bg_out,g_out,fr)\n",
        "    return g_out,fin_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEwYCeMWU9kj",
        "colab_type": "text"
      },
      "source": [
        "**Testing on the model structure** (for Debug)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1bs3_BmJ8N1",
        "colab_type": "code",
        "outputId": "efda3572-de66-44ef-c844-3b690fdfe6af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "print(\"Splat\")\n",
        "X = torch.rand(1,3,256,256)\n",
        "lrsplat = Lr_Splat(cm=1,sb=16,lb=8,bn=True,lris=256)\n",
        "out = lrsplat(X)\n",
        "print(list(out.size()))\n",
        "print(\"Local Features\")\n",
        "X = torch.rand(1, 64, 16, 16)\n",
        "lrlocalfeatures = Lr_LocalFeatures(cm=1,sb=16,lb=8,bn=True,lris=256)\n",
        "out = lrlocalfeatures(X)\n",
        "print(list(out.size()))\n",
        "print(\"Global Features\")\n",
        "X = torch.rand(2, 64, 16, 16)\n",
        "lrglobalfeatures = Lr_GlobalFeatures(cm=1,sb=16,lb=8,bn=True,lris=256)\n",
        "out = lrglobalfeatures(X)\n",
        "print(list(out.size()))\n",
        "print(\"Fusion and Bilterial Grid\")\n",
        "LrLocal = torch.rand(2, 64, 16, 16)\n",
        "LrGlobal = torch.rand(2,64)\n",
        "fusion = Fusion()\n",
        "bg = LinearPredict_BGrid(cm=1,lb=8)\n",
        "out = fusion(LrLocal,LrGlobal)\n",
        "bg_out = bg(out)\n",
        "print(list(bg_out.size()))\n",
        "print(\"GuideNN\")\n",
        "X = torch.rand(2, 3, 1080, 1920)\n",
        "guide = Guide_PointwiseNN(bn=True)\n",
        "g_out = guide(X)\n",
        "print(list(g_out.size()))\n",
        "print(\"Slice and Apply Coefficients\")\n",
        "slice_op = SliceNApply()\n",
        "bg_out = bg_out.cuda()\n",
        "g_out = g_out.cuda()\n",
        "X = X.cuda()\n",
        "out = slice_op(bg_out,g_out,X)\n",
        "print(list(out.size()))\n",
        "print(\"Net\")\n",
        "lr = torch.rand(2,3,256,256).cuda()\n",
        "fr = torch.rand(2, 3, 1080, 1920).cuda()\n",
        "net = Net(_cm=1,_sb=16,_lb=8,_bn=True,_lris=256).cuda()\n",
        "out = net(lr,fr)[1]\n",
        "print(out.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Splat\n",
            "[1, 64, 16, 16]\n",
            "Local Features\n",
            "[1, 64, 16, 16]\n",
            "Global Features\n",
            "[2, 64]\n",
            "Fusion and Bilterial Grid\n",
            "[2, 12, 8, 16, 16]\n",
            "GuideNN\n",
            "[2, 1080, 1920]\n",
            "Slice and Apply Coefficients\n",
            "[2, 3, 1080, 1920]\n",
            "Net\n",
            "torch.Size([2, 3, 1080, 1920])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZROavE543d3",
        "colab_type": "text"
      },
      "source": [
        "**Model Summary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qPh7ZVf4q-d",
        "colab_type": "code",
        "outputId": "b8a6ab15-4021-46df-b4fc-2697baf7d15e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "summary(net, input_size=[(3, 256, 256),(3,1080,1920)])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 8, 128, 128]             224\n",
            "              ReLU-2          [-1, 8, 128, 128]               0\n",
            "Convolutional_Layer-3          [-1, 8, 128, 128]               0\n",
            "            Conv2d-4           [-1, 16, 64, 64]           1,168\n",
            "       BatchNorm2d-5           [-1, 16, 64, 64]              32\n",
            "              ReLU-6           [-1, 16, 64, 64]               0\n",
            "Convolutional_Layer-7           [-1, 16, 64, 64]               0\n",
            "            Conv2d-8           [-1, 32, 32, 32]           4,640\n",
            "       BatchNorm2d-9           [-1, 32, 32, 32]              64\n",
            "             ReLU-10           [-1, 32, 32, 32]               0\n",
            "Convolutional_Layer-11           [-1, 32, 32, 32]               0\n",
            "           Conv2d-12           [-1, 64, 16, 16]          18,496\n",
            "      BatchNorm2d-13           [-1, 64, 16, 16]             128\n",
            "             ReLU-14           [-1, 64, 16, 16]               0\n",
            "Convolutional_Layer-15           [-1, 64, 16, 16]               0\n",
            "         Lr_Splat-16           [-1, 64, 16, 16]               0\n",
            "           Conv2d-17           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-18           [-1, 64, 16, 16]             128\n",
            "             ReLU-19           [-1, 64, 16, 16]               0\n",
            "Convolutional_Layer-20           [-1, 64, 16, 16]               0\n",
            "           Conv2d-21           [-1, 64, 16, 16]          36,928\n",
            "Convolutional_Layer-22           [-1, 64, 16, 16]               0\n",
            " Lr_LocalFeatures-23           [-1, 64, 16, 16]               0\n",
            "           Conv2d-24             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-25             [-1, 64, 8, 8]             128\n",
            "             ReLU-26             [-1, 64, 8, 8]               0\n",
            "Convolutional_Layer-27             [-1, 64, 8, 8]               0\n",
            "           Conv2d-28             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-29             [-1, 64, 4, 4]             128\n",
            "             ReLU-30             [-1, 64, 4, 4]               0\n",
            "Convolutional_Layer-31             [-1, 64, 4, 4]               0\n",
            "           Linear-32                  [-1, 256]         262,400\n",
            "      BatchNorm1d-33                  [-1, 256]             512\n",
            "             ReLU-34                  [-1, 256]               0\n",
            "Fully_Connected_Layer-35                  [-1, 256]               0\n",
            "           Linear-36                  [-1, 128]          32,896\n",
            "      BatchNorm1d-37                  [-1, 128]             256\n",
            "             ReLU-38                  [-1, 128]               0\n",
            "Fully_Connected_Layer-39                  [-1, 128]               0\n",
            "           Linear-40                   [-1, 64]           8,256\n",
            "Fully_Connected_Layer-41                   [-1, 64]               0\n",
            "Lr_GlobalFeatures-42                   [-1, 64]               0\n",
            "             ReLU-43           [-1, 64, 16, 16]               0\n",
            "           Fusion-44           [-1, 64, 16, 16]               0\n",
            "           Conv2d-45           [-1, 96, 16, 16]           6,240\n",
            "Convolutional_Layer-46           [-1, 96, 16, 16]               0\n",
            "LinearPredict_BGrid-47        [-1, 12, 8, 16, 16]               0\n",
            "           Conv2d-48       [-1, 16, 1080, 1920]              64\n",
            "      BatchNorm2d-49       [-1, 16, 1080, 1920]              32\n",
            "             ReLU-50       [-1, 16, 1080, 1920]               0\n",
            "Convolutional_Layer-51       [-1, 16, 1080, 1920]               0\n",
            "           Conv2d-52        [-1, 1, 1080, 1920]              17\n",
            "          Sigmoid-53        [-1, 1, 1080, 1920]               0\n",
            "Convolutional_Layer-54        [-1, 1, 1080, 1920]               0\n",
            "Guide_PointwiseNN-55           [-1, 1080, 1920]               0\n",
            "      SliceNApply-56        [-1, 3, 1080, 1920]               0\n",
            "================================================================\n",
            "Total params: 483,521\n",
            "Trainable params: 483,521\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 4665600.00\n",
            "Forward/backward pass size (MB): 1131.72\n",
            "Params size (MB): 1.84\n",
            "Estimated Total Size (MB): 4666733.57\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP4Fc1oeC-MT",
        "colab_type": "text"
      },
      "source": [
        "**Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyly63ivKiZQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import os\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class LowLightDataSet(Dataset):\n",
        "    def __init__(self, data_dir):\n",
        "        self.dataset_input_dir = os.path.join(data_dir, 'low')\n",
        "        self.dataset_gt_dir = os.path.join(data_dir, 'high')\n",
        "        self.list_of_files = self.listAllInputImageFiles(self.dataset_input_dir)\n",
        "        self.HighResTransform = transforms.Compose(\n",
        "            [\n",
        "                transforms.ToTensor()\n",
        "            ]\n",
        "        )\n",
        "        self.LowResTransform = transforms.Compose(\n",
        "           [\n",
        "            transforms.Resize((256,256)),\n",
        "            transforms.ToTensor()\n",
        "           ]\n",
        "        )\n",
        "        self.greyScaleTransform = transforms.Compose(\n",
        "            [transforms.Grayscale(1),\n",
        "             transforms.ToTensor()\n",
        "             ]\n",
        "       )\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_name = self.list_of_files[index]\n",
        "        hr_input_image_path = os.path.join(self.dataset_input_dir,image_name)\n",
        "        hr_gt_image_path = os.path.join(self.dataset_gt_dir,image_name)\n",
        "        with Image.open(hr_input_image_path) as img:\n",
        "            tmp_image = img.convert('RGB')\n",
        "            input_image_hr = self.HighResTransform(tmp_image)\n",
        "            input_image_lr = self.LowResTransform(tmp_image)\n",
        "        with Image.open(hr_gt_image_path) as img2:\n",
        "            tmp_image = img2.convert('RGB')\n",
        "            gt_image_hr = self.HighResTransform(tmp_image)\n",
        "            grey_image = self.greyScaleTransform(tmp_image)\n",
        "        return input_image_hr, input_image_lr, gt_image_hr, grey_image\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.list_of_files)\n",
        "\n",
        "    @staticmethod\n",
        "    def listAllInputImageFiles(data_dir):\n",
        "        list = os.listdir(data_dir)\n",
        "        files = []\n",
        "        for l in list:\n",
        "            fullpath = os.path.join(data_dir, l)\n",
        "            # FOR DEBUG: print(fullpath)\n",
        "            if os.path.isfile(fullpath):\n",
        "                files.append(l)\n",
        "        #files = sorted(files,key=lambda i: int(os.path.splitext(os.path.basename(i))[0]))\n",
        "        return files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AreHmLwoR_uv",
        "colab_type": "text"
      },
      "source": [
        "**Testing on input images** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20OV2Id8nhIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def showIMG(imgs):\n",
        "  for img in imgs:\n",
        "    plt.figure()\n",
        "    if img.shape[0] == 3:\n",
        "      img_np = img.permute(1,2,0).cpu().detach().numpy() \n",
        "      print(\"showIMG() Output\",img_np.shape)\n",
        "      plt.imshow(img_np,interpolation=\"bilinear\")\n",
        "\n",
        "    elif img.shape[0] == 1:\n",
        "      g_out_np = img.permute(1,2,0).cpu().detach().squeeze(2).numpy()\n",
        "      print(\"showIMG() Guide Map\",g_out_np.shape)\n",
        "      plt.imshow(g_out_np,cmap='gray',interpolation=\"bilinear\")\n",
        "\n",
        "def showResultMap(img):\n",
        "   plt.figure()\n",
        "   img_np = img.permute(1,2,0).cpu().detach().numpy() \n",
        "   img_np_s = (img_np[:,:,0] + img_np[:,:,1] +  img_np[:,:,2])/3\n",
        "   print(\"showResultMap() Output\",img_np_s.shape)\n",
        "   plt.imshow(img_np_s,interpolation=\"bilinear\", cmap=\"gray\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTkzdYE73-0A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# t_training_dataset = LowLightDataSet(\"/content/gdrive/My Drive/Data/RetinexNetData/LOLdataset/our485\")\n",
        "# i_fr, i_lr, o_fr = t_training_dataset[0]\n",
        "t_testing_dataset = LowLightDataSet(\"/content/gdrive/My Drive/Data/RetinexNetData/LOLdataset/eval15\")\n",
        "i_fr, i_lr, o_fr, grey_fr= t_testing_dataset[7]\n",
        "showIMG([i_fr, i_lr, o_fr,grey_fr])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XM9V8QnqPQkm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "showResultMap(i_fr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OUQ-SfAt5WR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "guide = Guide_PointwiseNN(bn=True)\n",
        "i_fr_ = i_fr.unsqueeze(0)\n",
        "g_out = guide(i_fr_)\n",
        "showIMG([g_out])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZVzC_AZ8nz9",
        "colab_type": "text"
      },
      "source": [
        "**Evaluation function and Loss Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOMC0BGi8lrd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_PSNR(output, target):\n",
        "  return 10 * torch.log10 (1/F.mse_loss(output, target))\n",
        "\n",
        "class MS_SSIM(nn.Module):\n",
        "   def __init__(self, window_size=11, size_average=True, normalize=True):\n",
        "        super(MS_SSIM, self).__init__()\n",
        "        self.window_size = window_size\n",
        "        self.size_average = size_average\n",
        "        self.normalize = normalize\n",
        "\n",
        "   def forward(self, output, target):\n",
        "        return msssim(output, target, window_size=self.window_size, size_average=self.size_average,\n",
        "                      normalize=self.normalize)\n",
        "        \n",
        "class SSIM(nn.Module):\n",
        "   def __init__(self, window_size=11, size_average=True):\n",
        "        super(SSIM, self).__init__()\n",
        "        self.window_size = window_size\n",
        "        self.size_average = size_average\n",
        "\n",
        "   def forward(self, output, target):\n",
        "        return ssim(output, target, window_size=self.window_size, size_average=self.size_average)\n",
        "\n",
        "          \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMbP9hCNY9I-",
        "colab_type": "text"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hji-jpRIQEdz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MODEL PARAMETERS and TRAINING PARAMETERS\n",
        "\n",
        "spatial_bins = 16\n",
        "luma_bins = 8\n",
        "channel_multiplier = 1 #ori 1\n",
        "low_res_input_size = 256\n",
        "epochs = 50\n",
        "weight_decay = 10 ** -8 #ori -8\n",
        "batch_size= 16\n",
        "batch_norm = True\n",
        "learning_rate = 10 ** -4 #ori -4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owWN829RP11N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(sb,lb,cm,lris,epochs,w_decay,bs,bn,learning_rate,isResume=True, colab=True, checkptfolder='/content/gdrive/My Drive/Checkpoint/1/',training_path = \"/content/gdrive/My Drive/Data/RetinexNetData/BrighteningTrain/\", checkptname='checkpt.pth',checkptinterval=10):\n",
        "    \n",
        "    CUDA=torch.cuda.is_available()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    if CUDA:\n",
        "        dl_pin_memory= True\n",
        "        device = torch.device(\"cuda\")\n",
        "    else:\n",
        "        dl_pin_memory= False\n",
        "        device = torch.device(\"cpu\")\n",
        "        return\n",
        "    print(\"Cuda\",dl_pin_memory)\n",
        "\n",
        "    if colab:\n",
        "      drive.mount('/content/gdrive')\n",
        "    \n",
        "    training_dataset = LowLightDataSet(training_path)\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=training_dataset, batch_size=bs,shuffle=True,pin_memory=dl_pin_memory)\n",
        "\n",
        "\n",
        "    model = Net(_cm=cm,_sb=sb,_lb=lb,_bn=bn,_lris=lris)\n",
        "    msssim_criterion = MS_SSIM() \n",
        "    ssim_criterion = SSIM()\n",
        "    mse_criterion = torch.nn.MSELoss()\n",
        "    l1_criterion = torch.nn.L1Loss()\n",
        "    optimizer = optim.Adam(model.parameters(),lr = learning_rate, weight_decay=w_decay)\n",
        "    curr_epoch = 0\n",
        "    losslogger = []\n",
        "\n",
        "    if isResume:\n",
        "      if os.path.isfile(checkptfolder+checkptname):\n",
        "        print(\"=> Loading checkpoint'{}\".format(checkptfolder+checkptname))\n",
        "        checkpt = torch.load(checkptfolder+checkptname)\n",
        "        curr_epoch = checkpt['epoch'] \n",
        "        epochs -= curr_epoch\n",
        "        model.load_state_dict(checkpt['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpt['optimizer_state_dict'])\n",
        "        losslogger = checkpt['losslogger']\n",
        "        for state in optimizer.state.values():\n",
        "          for k, v in state.items():\n",
        "              if isinstance(v, torch.Tensor):\n",
        "                 state[k] = v.cuda()\n",
        "        print(\"=> loaded checkpoint '{}' (start from epoch {})\".format(checkptfolder+checkptname, checkpt['epoch']))\n",
        "      else:\n",
        "        print(\"=> no checkpoint found at '{}'\".format(checkptfolder+checkptname))\n",
        "        return\n",
        "    else:\n",
        "      print(\"=> No checkpoint will be used\")\n",
        "      \n",
        "    model.to(device)\n",
        "\n",
        "    for e in range(epochs):\n",
        "      e_losslogger = []\n",
        "      model.train()\n",
        "\n",
        "      for batch_idx, (fr,lr,target,target_map) in enumerate(train_loader):\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        lr = lr.to(device)\n",
        "        fr = fr.to(device)\n",
        "        target_map = target_map.to(device).squeeze(1)\n",
        "        target = target.to(device)\n",
        "        g_out, output = model(lr, fr)\n",
        "        msssimloss = 1 - msssim_criterion(output, target)\n",
        "        ssimloss = 1 - ssim_criterion(output, target)\n",
        "        l1loss = l1_criterion(output, target)\n",
        "        l2loss = mse_criterion(output, target)\n",
        "        loss = msssimloss\n",
        "        e_losslogger.append(loss)\n",
        "        loss.backward()\n",
        "\n",
        "        print(\"Epoch: {}, Batch: {}, Loss: {}, SSIM: {}, MS-SSIM: {}, L1 Loss: {}, L2 Loss:{}, PSNR: {} dB\".format(curr_epoch,batch_idx, loss.item(),\n",
        "                                                                                                    ssim(output, target,  size_average=True), \n",
        "                                                                                                    msssim( output, target,  size_average=True),\n",
        "                                                                                                    l1loss.item(), l2loss.item(),\n",
        "                                                                                                    compute_PSNR(output,target)))\n",
        "        optimizer.step()\n",
        "\n",
        "      losslogger.append(e_losslogger)\n",
        "      if (curr_epoch+1) % checkptinterval ==  0:\n",
        "          saveCheckpt(checkptfolder,curr_epoch,model,optimizer,losslogger)\n",
        "      curr_epoch += 1\n",
        " \n",
        "def saveCheckpt(checkptfolder,curr_epoch,model,optimizer,losslogger,batch_idx='_'):\n",
        "  checkptname = 'checkpt_epoch_{}_batch_{}.pth'.format(curr_epoch,batch_idx)\n",
        "  print(\"Saving checkpoint {}\".format(checkptname))\n",
        "  torch.save({\n",
        "    'epoch': curr_epoch,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'losslogger': losslogger,\n",
        "    }, checkptfolder + checkptname)\n",
        "  print(\"{} is saved\".format(checkptname))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSXV2UaBD_wn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Train from scratch\n",
        "train(sb=spatial_bins,lb=luma_bins,cm=channel_multiplier,lris=low_res_input_size,epochs=epochs,\n",
        "      w_decay=weight_decay,bs=batch_size,bn=batch_norm,learning_rate=learning_rate,isResume=False,\n",
        "      training_path = \"/content/gdrive/My Drive/Data/RetinexNetData/LOLdataset/our485\",\n",
        "      checkptfolder='/content/gdrive/My Drive/Checkpoint/1/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFJZR8BJZ_Ix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Train from checkpoint\n",
        "train(sb=spatial_bins,lb=luma_bins,cm=channel_multiplier,lris=low_res_input_size,epochs=100,\n",
        "      w_decay=weight_decay,bs=batch_size,bn=batch_norm,learning_rate=learning_rate,\n",
        "      isResume=True,checkptname='checkpt_epoch_10_batch__.pth', checkptfolder='/content/gdrive/My Drive/Checkpoint/3/',\n",
        "      training_path = \"/content/gdrive/My Drive/Data/RetinexNetData/LOLdataset/our485\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fgc92M9zSLR2",
        "colab_type": "text"
      },
      "source": [
        "**Demo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nAGkZ5CStwh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def demo(input,isShowGuideMap=False, sb=16,lb=8,cm=1,lris=256,bn=True,checkptfolder='/content/gdrive/My Drive/Checkpoint/',checkptname='checkpt.pth', colab=True,testing_path = \"/content/gdrive/My Drive/Data/RetinexNetData/LOLdataset/eval15\"):\n",
        "\n",
        "  CUDA=torch.cuda.is_available()\n",
        "  torch.backends.cudnn.benchmark = True\n",
        "  torch.backends.cudnn.fastest = True\n",
        "\n",
        "  if CUDA:\n",
        "        dl_pin_memory= True\n",
        "        device = torch.device(\"cuda\")\n",
        "  else:\n",
        "        dl_pin_memory= False\n",
        "        device = torch.device(\"cpu\")\n",
        "        return\n",
        "  print(\"Cuda\",dl_pin_memory)\n",
        "\n",
        "  if colab:\n",
        "    drive.mount('/content/gdrive')\n",
        "  \n",
        "  model = Net(_cm=cm,_sb=sb,_lb=lb,_bn=bn,_lris=lris)\n",
        "\n",
        "  if os.path.isfile(checkptfolder+checkptname):\n",
        "      print(\"=> Loading checkpoint'{}\".format(checkptfolder+checkptname))\n",
        "      checkpt = torch.load(checkptfolder+checkptname)\n",
        "      model.load_state_dict(checkpt['model_state_dict'])\n",
        "      print(\"=> loaded checkpoint '{}' \".format(checkptfolder+checkptname))\n",
        "  else:\n",
        "      print(\"=> no checkpoint found at '{}'\".format(checkptfolder+checkptname))\n",
        "    \n",
        "  model.eval()\n",
        "  model.to(device)\n",
        "  lr, fr = loadIMG(input)\n",
        "  lr = lr.to(device)\n",
        "  fr = fr.to(device)\n",
        "  g_out,output = model(lr,fr)\n",
        "  imgs_output = []\n",
        "  imgs_output.append(output.squeeze(0))\n",
        "  if isShowGuideMap:\n",
        "     imgs_output.append(g_out)\n",
        "  showIMG(imgs_output) \n",
        "  #showResultMap(output.squeeze(0))\n",
        "\n",
        "def loadIMG(image_path):\n",
        "  fr_img_loader = transforms.Compose([transforms.ToTensor()])\n",
        "  lr_img_loader = transforms.Compose([transforms.Resize((256,256)),transforms.ToTensor()])\n",
        "  image = Image.open(image_path)\n",
        "  fr_image = fr_img_loader(image)\n",
        "  lr_image = lr_img_loader(image)\n",
        "  print(\"loadIMG()\",fr_image.shape)\n",
        "  fr_image = fr_image.unsqueeze(0) \n",
        "  lr_image = lr_image.unsqueeze(0) \n",
        "  return lr_image, fr_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4-6CMJeigXQ",
        "colab_type": "text"
      },
      "source": [
        "**Demo image**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1MFE_ZcieN3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testing_img = '/content/gdrive/My Drive/Data/RetinexNetData/LOLdataset/eval15/low/665.png'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RvZAUruwgBh",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2z8oVZukJKT",
        "colab_type": "text"
      },
      "source": [
        "**Train with LOL Dataset: cm = 1, loss = ms-ssim**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XJnTdXDkJrp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start = timer()\n",
        "demo(testing_img,checkptname='checkpt_epoch_49_batch__.pth',checkptfolder='/content/gdrive/My Drive/Checkpoint/LOL_CM1_MSSSIM/', cm=1, isShowGuideMap = True)\n",
        "end = timer()\n",
        "print(end - start,\"seconds\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxHeMY_uPrdb",
        "colab_type": "text"
      },
      "source": [
        "**Train with LOL Dataset: cm = 3, loss = ms-ssim**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNMC7UI-Pq1F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start = timer()\n",
        "demo(testing_img,checkptname='checkpt_epoch_49_batch__.pth',checkptfolder='/content/gdrive/My Drive/Checkpoint/LOL_CM3_MSSSIM/', cm=3, isShowGuideMap = True)\n",
        "end = timer()\n",
        "print(end - start,\"seconds\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YPuMa_MSSUy",
        "colab_type": "text"
      },
      "source": [
        "**Train with LOL Dataset: cm = 3, loss = l1_loss**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cOYVIeWSVz4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start = timer()\n",
        "demo(testing_img,checkptname='checkpt_epoch_49_batch__.pth',checkptfolder='/content/gdrive/My Drive/Checkpoint/LOL_CM3_L1/', cm=3, isShowGuideMap = True)\n",
        "end = timer()\n",
        "print(end - start,\"seconds\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y0qec30SQ1d",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZmAFZ3oh0z0",
        "colab_type": "text"
      },
      "source": [
        "**Train with LOL Dataset: cm = 3, loss = ms-ssim, l1, l2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26_Z2sY9RKt2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start = timer()\n",
        "demo(testing_img,checkptname='checkpt_epoch_59_batch__.pth',checkptfolder='/content/gdrive/My Drive/Checkpoint/LOL_cm3_ep49-99_mssimloss_l2loss_l1_loss/', cm=3, isShowGuideMap = True)\n",
        "end = timer()\n",
        "print(end - start,\"seconds\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMqb0VkQhzaL",
        "colab_type": "text"
      },
      "source": [
        "**Train with LOL Dataset: cm = 2, loss = ms-ssim, l1, l2**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GDPmOvb37qv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start = timer()\n",
        "demo(testing_img,checkptname='LOL_cm2_ep_49_msssimloss_l2loss_l1loss.pth',checkptfolder='/content/gdrive/My Drive/Checkpoint/lol_cm2_ep49_msssim_l1_l2/', cm=2,isShowGuideMap=True)\n",
        "end = timer()\n",
        "print(end - start,\"seconds\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yau6huU9kGPE",
        "colab_type": "text"
      },
      "source": [
        "**Train with LOL Dataset: cm = 1, loss = ms-ssim, l1, l2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2vgRYgokJCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start = timer()\n",
        "demo(testing_img,checkptname='checkpt_epoch_49_batch__.pth',checkptfolder='/content/gdrive/My Drive/Checkpoint/LOL_cm1_ep4849_msssimloss_l1loss_l2loss/', cm=1,isShowGuideMap=True)\n",
        "end = timer()\n",
        "print(end - start,\"seconds\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rd-HIZtwlts",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRxLFbTswo-L",
        "colab_type": "text"
      },
      "source": [
        "**Train with LOL Dataset: cm = 3, loss = l2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4P8MW_rue3k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start = timer()\n",
        "demo(testing_img,checkptname='checkpt_epoch_49_batch__.pth',checkptfolder='/content/gdrive/My Drive/Checkpoint/LOL_cm3_ep49-51_l2loss/', cm=3,isShowGuideMap=True)\n",
        "end = timer()\n",
        "print(end - start,\"seconds\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YIKMHQy0xAIB"
      },
      "source": [
        "**Train with LOL Dataset: cm = 3, loss = l2 , ms-ssim**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7tkuX8uxFHU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start = timer()\n",
        "demo(testing_img,checkptname='checkpt_epoch_49_batch__.pth',checkptfolder='/content/gdrive/My Drive/Checkpoint/LOL_cm3_ep47-49_msssimloss_l2_loss/', cm=3,isShowGuideMap=True)\n",
        "end = timer()\n",
        "print(end - start,\"seconds\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wmS8TamrxYAF"
      },
      "source": [
        "**Train with LOL Dataset: cm = 3, loss = ms-ssim, l1, l2**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8PyHtKcxi-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start = timer()\n",
        "demo(testing_img,checkptname='checkpt_epoch_59_batch__.pth',checkptfolder='/content/gdrive/My Drive/Checkpoint/LOL_cm3_ep49-99_mssimloss_l2loss_l1_loss/', cm=3, isShowGuideMap = True)\n",
        "end = timer()\n",
        "print(end - start,\"seconds\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGZ_F0q-chUn",
        "colab_type": "text"
      },
      "source": [
        "**Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-19cbrPvcjTV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(isShowIMG=False,isShowGuideMap=False, sb=16,lb=8,cm=1,lris=256,bn=True,checkptfolder='/content/gdrive/My Drive/Checkpoint/',checkptname='checkpt.pth', colab=True,testing_path = \"/content/gdrive/My Drive/Data/RetinexNetData/LOLdataset/eval15\"):\n",
        "\n",
        "  count = 0\n",
        "  totalmsssimloss = 0\n",
        "  totalssimloss = 0\n",
        "  totalPSNR = 0\n",
        "  totalL1Loss = 0\n",
        "  totalL2Loss = 0\n",
        "\n",
        "  CUDA=torch.cuda.is_available()\n",
        "  torch.backends.cudnn.benchmark = True\n",
        "  torch.backends.cudnn.fastest = True\n",
        "\n",
        "  if CUDA:\n",
        "        dl_pin_memory= True\n",
        "        device = torch.device(\"cuda\")\n",
        "  else:\n",
        "        dl_pin_memory= False\n",
        "        device = torch.device(\"cpu\")\n",
        "        return\n",
        "  print(\"Cuda\",dl_pin_memory)\n",
        "\n",
        "  testing_dataset = LowLightDataSet(testing_path)\n",
        "  test_loader = torch.utils.data.DataLoader(dataset=testing_dataset, batch_size=1,shuffle=True,pin_memory=dl_pin_memory)\n",
        "\n",
        "\n",
        "\n",
        "  if colab:\n",
        "    drive.mount('/content/gdrive')\n",
        "  \n",
        "  model = Net(_cm=cm,_sb=sb,_lb=lb,_bn=bn,_lris=lris)\n",
        "\n",
        "  if os.path.isfile(checkptfolder+checkptname):\n",
        "      print(\"=> Loading checkpoint'{}\".format(checkptfolder+checkptname))\n",
        "      checkpt = torch.load(checkptfolder+checkptname)\n",
        "      model.load_state_dict(checkpt['model_state_dict'])\n",
        "      print(\"=> loaded checkpoint '{}' \".format(checkptfolder+checkptname))\n",
        "  else:\n",
        "      print(\"=> no checkpoint found at '{}'\".format(checkptfolder+checkptname))\n",
        "      return\n",
        "\n",
        "  model.eval()\n",
        "  model.to(device)\n",
        "\n",
        "  for batch_idx, (fr,lr,target,target_map) in enumerate(test_loader):\n",
        "  \n",
        "    lr = lr.to(device)\n",
        "    fr = fr.to(device)\n",
        "    target = target.to(device)\n",
        "    g_out, output = model(lr,fr)\n",
        "    #output = amplification(output)\n",
        "    l1_criterion = torch.nn.L1Loss()\n",
        "    l1loss = l1_criterion(output, target)\n",
        "    l2_criterion = torch.nn.MSELoss()\n",
        "    l2loss = l2_criterion(output, target)\n",
        "    print(\"Image: {}, SSIM: {}, MS-SSIM: {}, L1 Loss: {}, L2 Loss:{}, PSNR: {} dB \".format(batch_idx,\n",
        "                                                                              ssim(output, target,  size_average=True), \n",
        "                                                                              msssim( output, target,  size_average=True),\n",
        "                                                                              l1loss.item(), l2loss.item(),\n",
        "                                                                              compute_PSNR(output,target)))\n",
        "    count += 1\n",
        "    totalssimloss += ssim(output, target,  size_average=True)\n",
        "    totalmsssimloss += msssim( output, target,  size_average=True)\n",
        "    totalPSNR += compute_PSNR(output,target)\n",
        "    totalL1Loss += l1loss.item()\n",
        "    totalL2Loss += l2loss.item()\n",
        "    imgs_output = []\n",
        "    imgs_output.append(output.squeeze(0))\n",
        "    if isShowIMG:\n",
        "      if isShowGuideMap:\n",
        "        imgs_output.append(target.squeeze(0))\n",
        "        imgs_output.append(fr.squeeze(0))\n",
        "        #imgs_output.append(target_map.squeeze(0))\n",
        "      showIMG(imgs_output) \n",
        "      #showResultMap(output.squeeze(0))\n",
        "\n",
        "  print(\"Average: SSIM: {}, MS-SSIM: {}, L1 Loss: {}, L2 Loss: {}, PSNR: {} dB \".format(totalssimloss/count, \n",
        "                                                                          totalmsssimloss/count,\n",
        "                                                                          totalL1Loss/count,\n",
        "                                                                          totalL2Loss/count, \n",
        "                                                                          totalPSNR/count))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RZUwxNAAMLf",
        "colab_type": "text"
      },
      "source": [
        "**6.2.4.3. Different Loss Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsFwvWhGlZDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "test(checkptname='checkpt_epoch_49_batch__.pth',checkptfolder='/content/gdrive/My Drive/Checkpoint/LOL_CM1_MSSSIM/', testing_path = \"/content/gdrive/My Drive/Data/RetinexNetData/LOLdataset/eval15\", cm=1, isShowGuideMap=True, isShowIMG=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlVBUPA8eMVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "test(checkptname='checkpt_epoch_49_batch__.pth',checkptfolder='/content/gdrive/My Drive/Checkpoint/LOL_CM1_L1/', testing_path = \"/content/gdrive/My Drive/Data/RetinexNetData/LOLdataset/eval15\", cm=1, isShowGuideMap=True, isShowIMG=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9jv1qROePEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "test(checkptname='checkpt_epoch_49_batch__.pth',checkptfolder='/content/gdrive/My Drive/Checkpoint/LOL_CM1_L2/', testing_path = \"/content/gdrive/My Drive/Data/RetinexNetData/LOLdataset/eval15\", cm=1, isShowGuideMap=True, isShowIMG=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_u0N9jllMbLk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "test(checkptname='checkpt_epoch_49_batch__.pth',checkptfolder='/content/gdrive/My Drive/Checkpoint/LOL_cm1_ep4849_msssimloss_l1loss_l2loss/', testing_path = \"/content/gdrive/My Drive/Data/RetinexNetData/LOLdataset/eval15\", cm=1, isShowGuideMap=True, isShowIMG=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4hikdr6AQDU",
        "colab_type": "text"
      },
      "source": [
        "***6.2.5.4.\tNumber of Channels***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONki1nllAWO3",
        "colab_type": "code",
        "outputId": "5194785f-3b86-4f03-9bd6-150c2f9ae0c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "test(checkptname='checkpt_epoch_49_batch__.pth',checkptfolder='/content/gdrive/My Drive/Checkpoint/LOL_CM3_L1/', testing_path = \"/content/gdrive/My Drive/Data/RetinexNetData/LOLdataset/eval15\", cm=3, isShowGuideMap=True, isShowIMG=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wDgHH7tAt_V",
        "colab_type": "code",
        "outputId": "557bdcb4-cad8-4b3b-d89b-9f799bb968d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "test(checkptname='checkpt_epoch_49_batch__.pth',checkptfolder='/content/gdrive/My Drive/Checkpoint/LOL_CM3_MSSSIM/', testing_path = \"/content/gdrive/My Drive/Data/RetinexNetData/LOLdataset/eval15\", cm=3, isShowGuideMap=True, isShowIMG=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kxu6UK_CA5fM",
        "colab_type": "code",
        "outputId": "0fe07a7f-d5cb-4e81-d02f-15e9d5558357",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "test(checkptname='checkpt_epoch_49_batch__.pth',checkptfolder='/content/gdrive/My Drive/Checkpoint/LOL_cm3_ep49-51_l2loss/', testing_path = \"/content/gdrive/My Drive/Data/RetinexNetData/LOLdataset/eval15\", cm=3, isShowGuideMap=True, isShowIMG=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g99JcR21Bci3",
        "colab_type": "code",
        "outputId": "3b28f851-db03-45ec-91a9-984108cabf49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "test(checkptname='checkpt_epoch_49_batch__.pth',checkptfolder='/content/gdrive/My Drive/Checkpoint/LOL_cm3_ep49-99_mssimloss_l2loss_l1_loss/', testing_path = \"/content/gdrive/My Drive/Data/RetinexNetData/LOLdataset/eval15\", cm=3, isShowGuideMap=True, isShowIMG=True)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IJM2diCCDMo",
        "colab_type": "text"
      },
      "source": [
        "**6.2.5.5.\tDifferent combination of loss functions and number of epochs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmFTxUGPBwd6",
        "colab_type": "code",
        "outputId": "38e0197d-1d77-4c89-9610-cd4c6f1f3157",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "test(checkptname='checkpt_epoch_69_batch__.pth',checkptfolder='/content/gdrive/My Drive/Checkpoint/LOL_cm3_ep49-99_mssimloss_l2loss_l1_loss/', testing_path = \"/content/gdrive/My Drive/Data/RetinexNetData/LOLdataset/eval15\", cm=3, isShowGuideMap=True, isShowIMG=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbW-Iy4sB04j",
        "colab_type": "code",
        "outputId": "6804be13-9bc6-498d-c086-f51f34cc8ef2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "test(checkptname='checkpt_epoch_79_batch__.pth',checkptfolder='/content/gdrive/My Drive/Checkpoint/LOL_cm3_ep49-99_mssimloss_l2loss_l1_loss/', testing_path = \"/content/gdrive/My Drive/Data/RetinexNetData/LOLdataset/eval15\", cm=3, isShowGuideMap=True, isShowIMG=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBUrs3awG1yG",
        "colab_type": "code",
        "outputId": "15cce692-b864-40e5-ce58-cc0cd23ba78f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "test(checkptname='checkpt_epoch_89_batch__.pth',checkptfolder='/content/gdrive/My Drive/Checkpoint/LOL_cm3_ep49-99_mssimloss_l2loss_l1_loss/', testing_path = \"/content/gdrive/My Drive/Data/RetinexNetData/LOLdataset/eval15\", cm=3, isShowGuideMap=True, isShowIMG=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fj1hEWNxB3uN",
        "colab_type": "code",
        "outputId": "7bf1fde7-9362-405b-c9c4-381eb15c5014",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "test(checkptname='checkpt_epoch_99_batch__.pth',checkptfolder='/content/gdrive/My Drive/Checkpoint/LOL_cm3_ep49-99_mssimloss_l2loss_l1_loss/', testing_path = \"/content/gdrive/My Drive/Data/RetinexNetData/LOLdataset/eval15\", cm=3, isShowGuideMap=True, isShowIMG=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}